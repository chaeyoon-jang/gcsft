{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79288257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import pandas as pd \n",
    "from utils.utils import read_jsonl, extract_hash_answer\n",
    "from utils.prompt_hub import get_confidence_prompt\n",
    "\n",
    "def ruler_eval_func(base_url, predict_url):\n",
    "    base = read_jsonl(base_url)\n",
    "    check = read_jsonl(predict_url)\n",
    "    predicts = []\n",
    "    targets = []\n",
    "    for b, predict in zip(base, check):\n",
    "        targets.append([p.lower() for p in predict['reference']])\n",
    "        if b['task_label'] == 'cwe' or b['task_label'] == 'fwe' or b['task_label'] == 'vt':\n",
    "            predicts.append([re.sub(r'[^a-zA-Z]', '', w).lower() for w in predict['completion'].split(\" \")])\n",
    "        elif b['task_label'] == 'niah_multikey_1' or b['task_label'] == 'niah_multikey_2' or b['task_label'] == 'niah_single_1' or b['task_label'] == 'niah_single_2':\n",
    "            predicts.append([re.sub(r'[^0-9]', '', predict['completion']).lower()]) \n",
    "        elif b['task_label'] == 'niah_multikey_3' or b['task_label'] == 'niah_single_3':\n",
    "            predicts.append([re.sub(r'[^a-zA-Z0-9-]', '', predict['completion']).lower()])\n",
    "        elif b['task_label'] == 'niah_multiquery' or b['task_label'] == 'niah_multivalue':\n",
    "            predicts.append([re.sub(r'[^0-9]', '', w).lower() for w in predict['completion'].split(\" \")])\n",
    "        else:\n",
    "            predicts.append([predict['completion'].lower()])\n",
    "    tf = []\n",
    "    #new_p = []\n",
    "    for b, p, t in zip(base, predicts, targets):\n",
    "        p = [x for x in p if x != '']\n",
    "        #new_p.append(p)\n",
    "        if 'qa' in b['task_label']:\n",
    "            if set(p).issubset(set(t)):\n",
    "                tf.append(1)\n",
    "            else:\n",
    "                tf.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                if set(p) == set(t):\n",
    "                    tf.append(1)\n",
    "                else:\n",
    "                    tf.append(0) \n",
    "            except:\n",
    "                print(p)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea2403",
   "metadata": {},
   "source": [
    "## Make Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a5f8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ruler training datasets\n",
    "ruler_4k_base = read_jsonl('../logs/llama/ruler_4k_train_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_4k_train.jsonl', f'../logs/llama/ruler_4k_train_seed_samples/outputs_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_train_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + \"</answer>\\n\\n\" +get_confidence_prompt('default')[:-12] for x in ruler_4k_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + \"</answer>\\n\\n\" +get_confidence_prompt('multi')[:-22] for x in ruler_4k_base],\n",
    "    'conf_label_single': [\"<confidence>\" + c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"<reasoning_confidence>N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})\n",
    "\n",
    "## ruelr validation datasets \n",
    "ruler_4k_base = read_jsonl('../logs/llama/ruler_4k_valid_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_4k_valid.jsonl', f'../logs/llama/ruler_4k_valid_seed_samples/outputs_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_valid_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('default')[:-12] for x in ruler_4k_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('multi')[:-22] for x in ruler_4k_base],\n",
    "    'conf_label_single': [\"<confidence>\" + c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"<reasoning_confidence>N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef442a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 14 Jan 2026\n",
      "\n",
      "You are an expert assistant that provides clear and helpful answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a reasoning assistant.\n",
      "Given a question, think step by step to arrive at the correct answer.\n",
      "First, provide your reasoning enclosed in <think> and </think> tags.\n",
      "Then, provide the final answer enclosed in <answer> and </answer> tags.\n",
      "The <answer> tag must contain only the final answer, with no additional explanation.\n",
      "\n",
      "In Professor Plum's biology class there are 40 students. Of those students, 80 percent have puppies. Of those who have puppies, 25% also have parrots. How many students have both puppies and parrots?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<think>First, we need to find out how many students have puppies. To do this, we multiply the total number of students by the percentage of students who have puppies. 40 students * 0.8 = 32 students have puppies.</think>\n",
      "\n",
      "<think>Next, we need to find out how many of those students who have puppies also have parrots. To do this, we multiply the number of students who have puppies by the percentage of those who also have parrots. 32 students * 0.25 = 8 students have both puppies and parrots.</think>\n",
      "\n",
      "<answer>8</answer>\n",
      "\n",
      "You are a confidence estimator.\n",
      "Given a question and a model answer, how confident are you that the answer is correct?\n",
      "Respond with a single integer from 0 to 100 enclosed in <confidence> and </confidence> tags.\n",
      "<confidence>100</confidence>\n"
     ]
    }
   ],
   "source": [
    "gsm_base = read_jsonl('../logs/llama/gsm_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "gsm_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_data = read_jsonl(f'../logs/llama/gsm_seed_samples/outputs_seed_{i}_parsed.jsonl')\n",
    "    gold_answer = [extract_hash_answer(x['gold_answer']) for x in temp_data]\n",
    "    predicted_answer = [x['parsed'].split(\"**Model's Final Answer is:** \")[-1] for x in temp_data]\n",
    "    temp_tf = [re.sub(r'[^0-9]', '', predicted_answer[i]).lower() == re.sub(r'[^0-9]', '', gold_answer[i]).lower() for i in range(len(temp_data))]\n",
    "    gsm_samples_tf.append(temp_tf)\n",
    "\n",
    "conf_labels = []\n",
    "for i in range(len(gsm_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += gsm_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "\n",
    "full_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in gsm_base],\n",
    "    'predicted_answer': [x['completion'] for x in gsm_base],\n",
    "    'target_answer': [extract_hash_answer(x['reference']) for x in gsm_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('default')[:-12] for x in gsm_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('multi')[:-22] for x in gsm_base],\n",
    "    'conf_label_single': [\"<confidence>\" + c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"<reasoning_confidence>\" + c + \"</reasoning_confidence>\\n<evidence_confidence>N/A</evidence_confidence>\"  for c in conf_labels],\n",
    "    'task_type': ['gsm' for _ in range(len(gsm_base))]})\n",
    "\n",
    "full_data = full_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "gsm_train_data = full_data.iloc[:int(len(full_data)*0.8)]\n",
    "gsm_valid_data = full_data.iloc[int(len(full_data)*0.8):].reset_index(drop=True)\n",
    "\n",
    "print(full_data['conf_input_single'][0] + full_data['conf_label_single'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5785bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler_train_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/ruler_4k_train.csv\", index=False)\n",
    "ruler_valid_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/ruler_4k_valid.csv\", index=False)\n",
    "\n",
    "gsm_train_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/gsm_train.csv\", index=False)\n",
    "gsm_valid_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/gsm_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590ead6",
   "metadata": {},
   "source": [
    "## Make Qwen3-8B training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b2156f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mruler_4k_samples_tf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m     conf_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)))\n\u001b[1;32m     41\u001b[0m ruler_valid_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m: [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ruler_4k_base],\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_answer\u001b[39m\u001b[38;5;124m'\u001b[39m: [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ruler_4k_base],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf_label_multi\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<reasoning_confidence>N/A</reasoning_confidence>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<evidence_confidence>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</evidence_confidence>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m conf_labels],\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ruler_4k_base))]})\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## ruler training datasets\n",
    "ruler_4k_base = read_jsonl('../logs/qwen/38b_ruler_seed_samples/train_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_8k_train.jsonl', f'../logs/qwen/38b_ruler_seed_samples/train_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_train_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('default')[:-12] for x in ruler_4k_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('multi')[:-22] for x in ruler_4k_base],\n",
    "    'conf_label_single': [\"<confidence>\" + c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"<reasoning_confidence>N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})\n",
    "\n",
    "## ruelr validation datasets \n",
    "ruler_4k_base = read_jsonl('../logs/qwen/38b_ruler_seed_samples/valid_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_8k_valid.jsonl', f'../logs/qwen/38b_ruler_seed_samples/valid_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_valid_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + \"</answer>\\n\\n\" +  get_confidence_prompt('default')[:-12] for x in ruler_4k_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + \"</answer>\\n\\n\" + get_confidence_prompt('multi')[:-22] for x in ruler_4k_base],\n",
    "    'conf_label_single': [\"<confidence>\" + c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"<reasoning_confidence>N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})\n",
    "\n",
    "print(ruler_valid_data['conf_input_single'][0] + ruler_valid_data['conf_label_single'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a6d5dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_jsonl('/mnt/home/chaeyun-jang/gcsft/logs/qwen/38b_ruler_seed_samples/valid_seed_1.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e761c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcsft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
