{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79288257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import pandas as pd \n",
    "from utils.utils import read_jsonl, extract_hash_answer\n",
    "from utils.prompt_hub import get_confidence_prompt\n",
    "\n",
    "def ruler_eval_func(base_url, predict_url):\n",
    "    base = read_jsonl(base_url)\n",
    "    check = read_jsonl(predict_url)\n",
    "    predicts = []\n",
    "    targets = []\n",
    "    for b, predict in zip(base, check):\n",
    "        targets.append([p.lower() for p in predict['reference']])\n",
    "        if b['task_label'] == 'cwe' or b['task_label'] == 'fwe' or b['task_label'] == 'vt':\n",
    "            predicts.append([re.sub(r'[^a-zA-Z]', '', w).lower() for w in predict['completion'].split(\" \")])\n",
    "        elif b['task_label'] == 'niah_multikey_1' or b['task_label'] == 'niah_multikey_2' or b['task_label'] == 'niah_single_1' or b['task_label'] == 'niah_single_2':\n",
    "            predicts.append([re.sub(r'[^0-9]', '', predict['completion']).lower()]) \n",
    "        elif b['task_label'] == 'niah_multikey_3' or b['task_label'] == 'niah_single_3':\n",
    "            predicts.append([re.sub(r'[^a-zA-Z0-9-]', '', predict['completion']).lower()])\n",
    "        elif b['task_label'] == 'niah_multiquery' or b['task_label'] == 'niah_multivalue':\n",
    "            predicts.append([re.sub(r'[^0-9]', '', w).lower() for w in predict['completion'].split(\" \")])\n",
    "        else:\n",
    "            predicts.append([predict['completion'].lower()])\n",
    "    tf = []\n",
    "    #new_p = []\n",
    "    for b, p, t in zip(base, predicts, targets):\n",
    "        p = [x for x in p if x != '']\n",
    "        #new_p.append(p)\n",
    "        if 'qa' in b['task_label']:\n",
    "            if set(p).issubset(set(t)):\n",
    "                tf.append(1)\n",
    "            else:\n",
    "                tf.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                if set(p) == set(t):\n",
    "                    tf.append(1)\n",
    "                else:\n",
    "                    tf.append(0) \n",
    "            except:\n",
    "                print(p)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea2403",
   "metadata": {},
   "source": [
    "## Make Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5f8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ruler training datasets\n",
    "ruler_4k_base = read_jsonl('../logs/llama/ruler_4k_train_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_4k_train.jsonl', f'../logs/llama/ruler_4k_train_seed_samples/outputs_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_train_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + get_confidence_prompt('default') for x in ruler_4k_base],\n",
    "    'conf_label_single': [c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})\n",
    "\n",
    "## ruelr validation datasets \n",
    "ruler_4k_base = read_jsonl('../logs/llama/ruler_4k_valid_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "ruler_4k_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_tf = ruler_eval_func('../data/processed/ruler_4k_valid.jsonl', f'../logs/llama/ruler_4k_valid_seed_samples/outputs_seed_{i}.jsonl')\n",
    "    ruler_4k_samples_tf.append(temp_tf)\n",
    "    \n",
    "conf_labels = []\n",
    "for i in range(len(ruler_4k_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += ruler_4k_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "    \n",
    "ruler_valid_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in ruler_4k_base],\n",
    "    'predicted_answer': [x['completion'] for x in ruler_4k_base],\n",
    "    'target_answer': [x['reference'] for x in ruler_4k_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + get_confidence_prompt('default') for x in ruler_4k_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + get_confidence_prompt('multi') for x in ruler_4k_base],\n",
    "    'conf_label_single': [c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [\"N/A</reasoning_confidence>\\n<evidence_confidence>\" + c + \"</evidence_confidence>\" for c in conf_labels],\n",
    "    'task_type': ['ruler' for _ in range(len(ruler_4k_base))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef442a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_base = read_jsonl('../logs/llama/gsm_seed_samples/outputs_base_argmax.jsonl')\n",
    "\n",
    "gsm_samples_tf = []\n",
    "for i in range(10):\n",
    "    temp_data = read_jsonl(f'../logs/llama/gsm_seed_samples/outputs_seed_{i}_parsed.jsonl')\n",
    "    gold_answer = [extract_hash_answer(x['gold_answer']) for x in temp_data]\n",
    "    predicted_answer = [x['parsed'].split(\"**Model's Final Answer is:** \")[-1] for x in temp_data]\n",
    "    temp_tf = [re.sub(r'[^0-9]', '', predicted_answer[i]).lower() == re.sub(r'[^0-9]', '', gold_answer[i]).lower() for i in range(len(temp_data))]\n",
    "    gsm_samples_tf.append(temp_tf)\n",
    "\n",
    "conf_labels = []\n",
    "for i in range(len(gsm_base)):\n",
    "    count = 0\n",
    "    for j in range(10):\n",
    "        count += gsm_samples_tf[j][i]\n",
    "    conf_labels.append(str(int(count*10)))\n",
    "\n",
    "full_data = pd.DataFrame({\n",
    "    'input_prompt': [x['input'] for x in gsm_base],\n",
    "    'predicted_answer': [x['completion'] for x in gsm_base],\n",
    "    'target_answer': [extract_hash_answer(x['reference']) for x in gsm_base],\n",
    "    'conf_input_single': [x['input'] + x['completion'] + get_confidence_prompt('default') for x in gsm_base],\n",
    "    'conf_input_multi': [x['input'] + x['completion'] + get_confidence_prompt('multi') for x in gsm_base],\n",
    "    'conf_label_single': [c + \"</confidence>\" for c in conf_labels],\n",
    "    'conf_label_multi': [c + \"</reasoning_confidence>\\n<evidence_confidence>N/A</evidence_confidence>\"  for c in conf_labels],\n",
    "    'task_type': ['gsm' for _ in range(len(gsm_base))]})\n",
    "\n",
    "full_data = full_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "gsm_train_data = full_data.iloc[:int(len(full_data)*0.8)]\n",
    "gsm_valid_data = full_data.iloc[int(len(full_data)*0.8):].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5785bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler_train_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/ruler_4k_train.csv\", index=False)\n",
    "ruler_valid_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/ruler_4k_valid.csv\", index=False)\n",
    "\n",
    "gsm_train_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/gsm_train.csv\", index=False)\n",
    "gsm_valid_data.to_csv(\"../data/train_data/Llama-3.2-3B-Instruct/csft/gsm_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590ead6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcsft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
